import logging
import knime.extension as knext
import social_science_ext

LOGGER = logging.getLogger(__name__)


@knext.node(
    name="Factor Predictor",
    node_type=knext.NodeType.PREDICTOR,
    icon_path="icons/FactorPredictor.png",
    category=social_science_ext.main_category,
    keywords=[
        "Factor Scores",
        "PCA Transform",
        "Factor Predictor",
        "Component Scores",
        "Dimensionality Reduction",
    ],
    id="factor_predictor",
)
@knext.input_table(
    name="Input Data",
    description="Dataset to transform using trained factor model. Must contain all numerical variables used during model training. Missing values handled via listwise deletion.",
)
@knext.input_binary(
    name="Model",
    description="Trained factor analysis model with rotation matrices, scaling parameters, and transformation components. Generated by Factor Analysis learner node.",
    id="factor_analysis.model",
)
@knext.output_table(
    name="Factor Scores",
    description="Original data with appended factor/component scores. Scores maintain orthogonality properties and rotation consistency from trained model.",
)
class FactorScorerNode:
    """
    ## Factor Analysis Model Predictor

    Applies trained factor analysis models to new datasets, computing factor/component scores while maintaining mathematical consistency with the original rotation method and orthogonality properties.

    ## Scoring Methodology

    The node implements the following scoring process:

    - **Standardization Consistency**: Uses identical scaling parameters from training
    - **Rotation Matrix Application**: Applies saved rotation transformations to maintain interpretability
    - **Orthogonality Preservation**: Components remain orthogonal regardless of rotation method through proper matrix operations
    - **Method Consistency**: Handles PCA and Factor Analysis models with appropriate variance adjustments

    **Orthogonal Rotation Process**:

    1. **Whitening**: Unrotated scores are standardized to unit variance: **S_w = S_unrot / √λ**
    2. **Rotation**: Orthogonal matrix **R** applied: **S_final = S_w × R**
    3. **Orthogonality**: Since **R'R = I**, final scores remain uncorrelated: **Cov(S_final) = I**

    **Oblique Rotation Handling**:
    For oblique rotations (Promax), the rotation matrix allows factor correlation while maintaining consistent transformation geometry.

    ## Scoring Process

    1. **Data Validation**: Verifies all training features are present in input data
    2. **Missing Value Handling**: Removes observations with incomplete feature data
    3. **Standardization**: Applies training-time scaling parameters if model used standardization
    4. **Base Transformation**: Computes unrotated scores using trained model
    5. **Variance Standardization**: **PCA Methods**: Uses eigenvalues for proper scaling. **Factor Analysis**: Uses empirical factor variances
    6. **Rotation Application**: Applies saved rotation matrix to maintain interpretation consistency
    7. **Score Output**: Generates factor/component scores with consistent column naming

    ## Output Interpretation

    - **Orthogonal Methods**: Scores are uncorrelated (orthogonal) regardless of rotation
    - **Oblique Methods**: Scores may be correlated, reflecting factor relationships
    - **Scale Consistency**: Score magnitudes reflect original model's variance structure
    - **Interpretation**: Maintains same factor meanings as training model

    ## Configuration Options

    - **Component Count**: Specify number of factors/components to output (≤ training model components)

    ## Use Cases

    - **Model Application**: Apply trained dimensionality reduction to new datasets
    - **Prediction Pipeline**: Transform features for downstream modeling
    - **Consistent Scoring**: Maintain identical factor structure across data batches
    - **Research Validation**: Apply validated factor structures to new samples

    ## References

    - Jolliffe, I. T. (2002). *Principal Component Analysis* (2nd ed.). Springer
    - Kaiser, H. F. (1958). The varimax criterion for analytic rotation in factor analysis. *Psychometrika*, 23(3), 187–200
    - Harman, H. H. (1976). *Modern Factor Analysis* (3rd ed.). University of Chicago Press
    - Bartholomew, D. J., et al. (2011). *Analysis of Multivariate Social Science Data* (2nd ed.). CRC Press
    """

    n_components = knext.IntParameter(
        label="Number of Components to output",
        description="Specify how many factors/components to compute.",
        default_value=2,
        min_value=1,
        max_value=1000,
    )

    def configure(
        self,
        configure_context: knext.ConfigurationContext,
        input_schema: knext.Schema,
        input_model: bytes,
    ):
        """
        Defines the output schema by appending factor/component columns to the input schema.
        """
        input_column_names = [col.name for col in input_schema]
        input_column_types = [col.ktype for col in input_schema]

        fc_column_names = [f"FC{i + 1}" for i in range(self.n_components)]
        fc_column_types = [knext.double()] * self.n_components

        all_column_names = input_column_names + fc_column_names
        all_column_types = input_column_types + fc_column_types

        return (knext.Schema(all_column_types, all_column_names),)

    def execute(self, exec_context, input_table, model_binary):
        """
        Applies the trained factor analysis model to the input data and outputs factor/component scores.

        Parameters
        ----------
        exec_context : knext.ExecutionContext
            The KNIME execution context.
        input_table : knext.Table
            The input data table.
        model_binary : bytes
            The pickled factor analysis model object from the Factor Analyzer node.

        Returns
        -------
        knext.Table
            The input data with appended factor/component score columns.
        """
        # Import heavy dependencies only when needed
        import pickle
        import pandas as pd
        import numpy as np

        # Load the trained model and parameters from the comprehensive model dictionary
        model_data = pickle.loads(model_binary)

        # Extract model components
        factor_model = model_data["model"]  # The fitted sklearn model
        model_n_components = model_data["n_components"]  # Number of components trained

        # Extract preprocessing information
        scaler_mean = model_data["scaler_mean"]  # Feature means (if standardized)
        scaler_scale = model_data["scaler_scale"]  # Feature scales (if standardized)
        features_cols = model_data["features_cols"]  # Original feature column names

        # Extract rotation information
        rotation_matrix = model_data.get("rotation_matrix")  # Rotation transformation matrix
        rotation_method = model_data.get("rotation_method", "None")  # Rotation method used

        max_dims = self.n_components

        df = input_table.to_pandas()

        # Ensure all required feature columns are present
        missing_columns = [col for col in features_cols if col not in df.columns]
        if missing_columns:
            raise ValueError(f"Input table is missing required columns: {missing_columns}")

        # Mask for complete feature rows
        features_complete_mask = df[features_cols].notna().all(axis=1)

        # Filter feature matrix and all columns consistently
        X = df.loc[features_complete_mask, features_cols].reset_index(drop=True)
        result_df = df.loc[features_complete_mask].reset_index(drop=True).copy()

        # Check if trained model has enough components
        if model_n_components < max_dims:
            raise ValueError(f"Requested {max_dims} components, but the model was trained with only {model_n_components} components.")

        # Standardize using training scaler parameters if available
        if scaler_mean is not None and scaler_scale is not None:
            x_scaled = (X.values - scaler_mean) / scaler_scale
        else:
            x_scaled = X.values

        # Compute unrotated scores using the appropriate model
        scores_unrot = factor_model.transform(x_scaled)  # shape: (n_samples, n_fitted_components)
        scores_unrot = scores_unrot[:, :model_n_components]

        # Handle variance standardization based on analysis method
        analysis_method = model_data["analysis_method"]

        if rotation_method and analysis_method in ["STANDARD", "INCREMENTAL"]:
            # For PCA methods: use true eigenvalues (UNCHANGED - working correctly)
            eigvals = np.array(factor_model.explained_variance_[:model_n_components], dtype=float)
            eigvals[eigvals <= 0] = np.finfo(float).eps  # Defensive guard
            # Whiten: make unrotated scores unit-variance and uncorrelated
            s_whitened = scores_unrot / np.sqrt(eigvals)
        elif rotation_method and analysis_method == "FACTOR_ANALYSIS":
            # For Factor Analysis: standardize by factor variances to ensure orthogonality
            # Compute factor variances from unrotated scores
            factor_variances = np.var(scores_unrot, axis=0, ddof=1)
            factor_variances[factor_variances <= 0] = np.finfo(float).eps  # Defensive guard
            # Whiten: make unrotated scores unit-variance for proper rotation
            s_whitened = scores_unrot / np.sqrt(factor_variances)
        else:
            # For no rotation: use scores as-is
            s_whitened = scores_unrot

        # Apply rotation if present
        if rotation_matrix is None or rotation_method == "None":
            # NO_ROTATION: use whitened scores as-is
            scores = s_whitened
        else:
            # For all methods: Apply rotation matrix to whitened scores
            # This ensures consistent handling and proper orthogonality
            rotation_matrix = rotation_matrix[: s_whitened.shape[1], :max_dims]
            scores = s_whitened @ rotation_matrix

        # Build output table with appropriate column naming
        # Use FC (Factor Component) as a general term that works for both PCA and EFA
        # Create columns based on n_components and drop excess columns after scoring
        score_columns = [f"FC{i + 1}" for i in range(max_dims)]
        
        # Drop excess columns if scores has more columns than requested
        if scores.shape[1] > max_dims:
            scores = scores[:, :max_dims]
        
        scores_df = pd.DataFrame(scores, columns=score_columns)
        final_df = pd.concat([result_df, scores_df], axis=1).reset_index(drop=True)

        return knext.Table.from_pandas(final_df)